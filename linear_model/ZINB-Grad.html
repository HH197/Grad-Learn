<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ZINB-Grad: A Gradient Based Linear Model Outperforming Deep Models &mdash; Grad-Learn 0.1.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=01f34227"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Grad-Learn
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dataset/index.html">Dataset Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">Linear Models Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/index.html">Utility Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../API.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Grad-Learn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ZINB-Grad: A Gradient Based Linear Model Outperforming Deep Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/linear_model/ZINB-Grad.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="zinb-grad-a-gradient-based-linear-model-outperforming-deep-models">
<h1>ZINB-Grad: A Gradient Based Linear Model Outperforming Deep Models<a class="headerlink" href="#zinb-grad-a-gradient-based-linear-model-outperforming-deep-models" title="Link to this heading"></a></h1>
<p>scRNA-seq experiments are powerful, but they suffer from technical noise, dropouts, batch effects, and biases (See this
<a class="reference external" href="https://github.com/HH197/Deep-Generative-Modeling-and-Probabilistic-Dimension-Reduction#challenges-in-analyzing-single-cell-rna-eq-data">link</a>
for more detail).</p>
<p>Many statistical and machine learning methods have been designed to overcome these challenges. In recent years, there
has been a shift from traditional statistical models to deep learning models. But are deep models better for scRNA-seq
data analysis?</p>
<p>Published literature claimed that deep-learning-based models, such as scVI, outperform conventional models, such as
ZINB-WaVE. Here, we used a novel optimization procedure combined with modern machine learning software packages to
overcome the scalability and efficiency challenges inherited in traditional tools. We showed that our implementation
is more efficient than both conventional models and deep learning models.</p>
<p>We assessed our proposed model, ZINB-Grad, and compared it with <a class="reference external" href="https://www.nature.com/articles/s41592-018-0229-2">scVI</a>
and <a class="reference external" href="https://www.nature.com/articles/s41467-017-02554-5">ZINB-WaVE</a>, both developed at the UC Berkeley, using a set of
benchmarks, including run-time, goodness-of-fit, imputation error, clustering accuracy, and batch correction.</p>
<p>Our development shows that a conventional model optimized with the proper techniques and implemented using the right
tools can outperform state-of-the-art deep models. It can generalize better for unseen data; it is more interpretable,
and not surprisingly, it uses extremely fewer resources compared to deep models.</p>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#zinb-grad--a-gradient-based-linear-model-outperforming-deep-models"><span class="xref myst">ZINB-Grad: A Gradient Based Linear Model Outperforming Deep Models</span></a></p>
<ul>
<li><p><a class="reference internal" href="#table-of-contents"><span class="xref myst">Table of Contents</span></a></p></li>
<li><p><a class="reference internal" href="#project-organization"><span class="xref myst">Project Organization</span></a></p></li>
<li><p><a class="reference internal" href="#zinb-wave"><span class="xref myst">ZINB-WaVE</span></a></p>
<ul>
<li><p><a class="reference internal" href="#zinb-wave-bottleneck"><span class="xref myst">ZINB-WaVE Bottleneck</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#zinb-grad"><span class="xref myst">ZINB-Grad</span></a></p></li>
<li><p><a class="reference internal" href="#results-and-model-evaluation"><span class="xref myst">Results And Model Evaluation</span></a></p>
<ul>
<li><p><a class="reference internal" href="#run-time"><span class="xref myst">Run-time</span></a></p></li>
<li><p><a class="reference internal" href="#generalization"><span class="xref myst">Generalization</span></a></p></li>
<li><p><a class="reference internal" href="#imputation-evaluation"><span class="xref myst">Imputation Evaluation</span></a></p></li>
<li><p><a class="reference internal" href="#clustering"><span class="xref myst">Clustering</span></a></p></li>
<li><p><a class="reference internal" href="#batch-effect-correction"><span class="xref myst">Batch Effect Correction</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion"><span class="xref myst">Conclusion</span></a></p></li>
</ul>
</li>
</ul>
</section>
<section id="zinb-wave">
<h2>ZINB-WaVE<a class="headerlink" href="#zinb-wave" title="Link to this heading"></a></h2>
<p>ZINB-WaVE is a generalized linear mixed model (GLMM) which captures the technical variability through two known random variables and maps the data onto a biologically meaningful low-dimensional representation through a linear transformation. ZINB-WaVE extracts low-dimensional representation from the data by considering dropouts, over-dispersion, batch effects, and the count nature of the data.</p>
<img  src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-017-02554-5/MediaObjects/41467_2017_2554_Fig1_HTML.png?as=webp"> 
<p>In the above schematic graph, $X$ and $V$ are known sample-level and gene-level covariates, respectively. $X$ can model wanted or unwanted variations. For instance, $X$ can be used to correct batch effects, and $V$ can model gene length or GC-content. This is how ZINB-WaVE performs normalization and batch effect correction all in one step.</p>
<p>$W$ and $\alpha$ are the unknown matrices that are used for dimension reduction. Columns of $W$ are essentially the latent space of ZINB-WaVE, and $\alpha$ is its corresponding matrix of regression parameters. The $O$ parameters are the known offsets for $\pi$ and $\mu$.  For more details, please refer to <a class="reference external" href="https://www.nature.com/articles/s41467-017-02554-5">ZINB-WaVE</a>.</p>
<section id="zinb-wave-bottleneck">
<h3>ZINB-WaVE Bottleneck<a class="headerlink" href="#zinb-wave-bottleneck" title="Link to this heading"></a></h3>
<p>The bottleneck of the ZINB-WaVE is in its estimation process, which includes several iterative procedures, such as ridge regression and logistic regression. It also contains SVD decompositions of big matrices (for large sample sizes) and the BFGS quasi-Newton method. Not surprisingly, the optimization procedure requires heavy computations and high-memory storage. Therefore, the ZINB-WaVE is doomed to be scalable to only a few thousand cells, and nowadays, applications require millions of samples to be efficiently analyzed.</p>
<p>However, this limitation does not imply that the traditional linear models are suboptimal compared to deep learning-based models. Indeed, we will show that, by switching the optimization procedure from the conventional analytic way to a modern method, ZINB-WaVE is scalable to millions of cells and even more efficient and productive than its deep learning descendent, such as scVI.</p>
</section>
</section>
<section id="zinb-grad">
<h2>ZINB-Grad<a class="headerlink" href="#zinb-grad" title="Link to this heading"></a></h2>
<p>Supported by most modern machine learning packages (e.g., Pytorch and Tensorflow), gradient descent and its variants are used to optimize deep neural networks and are among the most popular algorithms for optimization. By updating the parameters in the opposite direction of the objective function’s gradient, gradient descent finds local optimum values of the model’s parameters to minimize the objective function regardless of how complicated is the model.</p>
<p>However, a neglected fact is that gradient descent may also be applied to “simpler” linear models when appropriate. Towards this line, we developed a gradient descent-based stochastic optimization process for the ZINB-WaVE to overcome the scalability and efficiency challenges inherited in its optimization procedure. We combined the new optimization method with modern statistical and machine learning libraries, which resulted in the ZINB-Grad, a gradient-based ZINB GLMM with GPU acceleration, high-performance scalability, and memory-efficient estimation.</p>
</section>
<section id="results-and-model-evaluation">
<h2>Results And Model Evaluation<a class="headerlink" href="#results-and-model-evaluation" title="Link to this heading"></a></h2>
<p>We evaluated ZINB-Grad using a set of benchmarking data sets and a range of technical and biological tests. We used three benchmarking datasets, namely, <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/25700174/">CORTEX</a>, <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/27565351/">RETINA</a>, and <a class="reference external" href="https://support.10xgenomics.com/single-cell-gene-expression/datasets">BRAIN</a>.</p>
<section id="run-time">
<h3>Run-time<a class="headerlink" href="#run-time" title="Link to this heading"></a></h3>
<p>Since Lopez <em>et al.</em> have clearly shown that <a class="reference external" href="https://www.nature.com/articles/s41592-018-0229-2">scVI</a> is significantly faster than <a class="reference external" href="https://www.nature.com/articles/s41467-017-02554-5">ZINB-WaVE</a> for comparable data set sizes, and for &gt; 30,000 samples, ZINB-WaVE cannot scale, we did not test ZINB-WaVE’s run-time.</p>
<p>Our analyses showed that ZINB-Grad run-time is exceedingly lower than the scVI for any data size (the following plot) due to the simplicity of the model. All scVI and ZINB-Grad tests were performed using a computer equipped with a V100 GPU and 32 GB of system memory for GPU acceleration.</p>
<p align="center">
<img width="600" height="400" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/train_time.png">
</p>
</section>
<section id="generalization">
<h3>Generalization<a class="headerlink" href="#generalization" title="Link to this heading"></a></h3>
<p>We examined the ZINB-WaVE goodness-of-fit for the train data as a reference for our model (ZINB-WaVE cannot scale to &gt; 30,000 sample). ZINB-Grad has the same (or even better) negative log-likelihood compared to ZINB-WaVE for various data sizes (below figure). Therefore, our optimization procedure will get a minimum of comparable quality to ZINB-WaVE’s estimation process. The train negative log-likelihood for ZINB-Grad is the same or better compared to scVI.</p>
<p align="center">
<img width="600" height="300" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/train_neg_loglik.png">
</p>
<p>Moreover, the negative log-likelihood of the validation set for ZINB-Grad is better than scVI in any data sizes (following figure), showing that our linear model generalizes (extrapolates) better than a deep model even for large sample sizes.</p>
<p align="center">
<img width="600" height="300" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/test_neg_loglik.png" >
</p>
</section>
<section id="imputation-evaluation">
<h3>Imputation Evaluation<a class="headerlink" href="#imputation-evaluation" title="Link to this heading"></a></h3>
<p>We corrupted the data by randomly selecting 10% of the non-zero entries and altering them to zero. Then, we used the median of the ${\mathbb L}_1$ distance between the original and imputed values of the altered data set as the accuracy for data imputation.</p>
<p>We corrupted the CORTEX data set, and then, we estimated the parameters of the models using the corrupted data set. Finally, we compared the original data set (before corruption) with the imputed data from the models trained with the corrupted data set using the median ${\mathbb L}_1$ distance. The following figure shows the performance of the three models in terms of imputation error. ZINB-Grad performance is comparable with scVI and is better than ZINB-WaVE.</p>
<p align="center">
<img width="500" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/impuation.png">
</p>
</section>
<section id="clustering">
<h3>Clustering<a class="headerlink" href="#clustering" title="Link to this heading"></a></h3>
<p>We performed clustering on the latent space with 10 dimensions for scVI, ZINB-WaVE, and ZINB-Grad using K-means. We calculated the Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI) between the gold standard labels of the CORTEX data set and labels obtained from K-means, along with Average Silhouette Width (ASW) to assess the clustering performance of the ZINB-Grad compared to scVI and ZINB-WaVE (below figure). For all scores, NMI, ARI, and ASW, the higher is better.</p>
<p align="center">
<img width="500" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/clustering.png">
</p>
<p>ZINB-Grad and ZINB-WaVE scores are close, and the clustering scores show that ZINB-Grad performed slightly better than scVI.</p>
</section>
<section id="batch-effect-correction">
<h3>Batch Effect Correction<a class="headerlink" href="#batch-effect-correction" title="Link to this heading"></a></h3>
<p>We evaluated the accountability for technical variability by assessing batch entropy of mixing and visualizing the latent space in the RETINA data set containing two batches. We performed two experiments to visualize the effect of batch correction. In one experiment, we corrected the batch effect; in the other one, we did not.</p>
<ul class="simple">
<li><p>The following figure show the latent space of the ZINB-Grad when batch annotations are not considered (Blue dots are Batch 1 and Green dots are Batch 2):</p></li>
</ul>
<p align="center">
<img width="600" height="600" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/batch_ncorrected.png">
</p>
<ul class="simple">
<li><p>The different clusters (cell types)  when batch annotations are not modeled:</p></li>
</ul>
<p align="center">
<img width="600" height="600" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/batch_ncorrected_clusters.png">
</p>
<p>These graphs show clearly that without performing batch correction, the technical variability will cause the cells in the same cluster (cell population) to construct different clusters, which will be misleading in the downstream analysis.</p>
<ul class="simple">
<li><p>The following figure show the latent space of the ZINB-Grad when batch annotations are considered (Blue dots are Batch 1 and Green dots are Batch 2):</p></li>
</ul>
<p align="center">
<img width="600" height="600" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/batch_corrected.png">
</p>
<ul class="simple">
<li><p>The different clusters (cell types)  when batch annotations are modeled:</p></li>
</ul>
<p align="center">
<img width="600" height="600" src="https://raw.githubusercontent.com/HH197/ZINB-Grad/main/figures/batch_corrected_clusters.png">
</p>
<p>These graphs show that after considering batch annotations, ZINB-Grad accounts for the technical variability and results in a biologically meaningful latent space.</p>
<p>We used the entropy of batch mixing to measure the batch effect correction. We randomly selected 100 cells from batches and found 50 nearest neighbors of each randomly chosen cell to calculate the average regional Shanon entropy of all 100 cells. The procedure is repeated for 100 iterations, and the average of the iterations is considered as the batch mixing score. We could not use ZINB-WaVE for the RETINA data set as it was too large for ZINB-WaVE to handle. The batch mixing scores for scVI and ZINB-Grad are 0.64 and 0.54, respectively.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>We showed that our implementation is more efficient than both conventional models and deep learning models. Moreover, our model is scalable to millions of samples and has performance comparable with deep models in terms of accuracy. As the devil is in the implementation details, the supremacy of deep models may not be due to their sophisticated deep architecture. Instead, the source of effectiveness is merely the optimization procedure built-in to deep models
implementations, which could also be adopted by many traditional models.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Hamid Hamidi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>